%!TEX root = ./thesis.tex
% $Log: abstract.tex,v $
% Revision 1.1  93/05/14  14:56:25  starflt
% Initial revision
% 
% Revision 1.1  90/05/04  10:41:01  lwvanels
% Initial revision
% 
%
%% The text of your abstract and nothing else (other than comments) goes here.
%% It will be single-spaced and the rest of the text that is supposed to go on
%% the abstract page will be generated by the abstractpage environment.  This
%% file should be \input (not \include 'd) from cover.tex.
\begin{abstractger}
In den letzten Jahren hat sich maschinelles Lernen als essentieller Bestandteil vieler erfolgreicher Anwendungen und Unternehmen etabliert.
W\"ahrend sich der Anstieg im Einsatz k\"unstlicher Intelligenz fortsetzt, steigt das zu verarbeitende Datenvolumen noch um ein vielfaches schneller.
Es ist daher nicht verwunderlich, dass sich ein Gro{\ss}teil der Forschung in diesem Bereich auf die effiziente Parallelisierung von Algorithmen konzentriert und aus diesem Grund neue Systeme sowie Frameworks ver\"offentlicht wurden, die versuchen Leistung und Effizienz von verteiltem maschinellem Lernen zu verbessern.
Trotz der gro{\ss}en Fortschritte haben viele der modernen Systeme noch Einschr\"ankungen.
Aktuellen Systemen mangelt es sowohl an Ausdruckskraft als auch an Flexibilit\"at, was sie f\"ur die Entwicklung und das schnelle Experimentieren mit Algorithmen des verteilen maschinellen Lernens unbrauchbar macht obwohl dies ein Hauptaugenmerk f\"ur optimale Leistung ist.
Auf der anderen Seite nutzen viele der Parallelisierungsstrategien die stochastischen Eigenschaften von Algorithmen des maschinellen Lernens um eine Parallelisierung zu erm\"oglichen.
Allerdings geht dies zu Lasten der Konsistenz zwischen den Partitionen eines verteilten Zustands.
Verringerte Konsistenz hat nicht zwangsl\"aufig einen negativen Einfluss auf das Ergebnis, kann aber unter Umst\"anden zu nicht optimaler Konvergenz f\"uhren, was wiederum f\"ur eine l\"angere Laufzeit verantwortlich ist.
Diese Masterarbeit pr\"asentiert den Prototypen eines Frameworks f\"ur verteiltes maschinelles Lernen, genannt ExaML, welches eine zustandszentrierte Programmierabstraktion verwendet.
Das Resultat ist ein Programmiermodell welches es dem Benutzer erlaubt sich auf die wichtigen Elemente bei der Entwicklung von Algorithmen des verteilten maschinellen Lernens zu konzentrieren.
Dies betrifft haupts\"achlich die Kommunikation von Fortschritt und Konsistenzmanagement zwischen verteilten Maschinen.
Zus\"atzlich \"ubernimmt das Framework die Aufgabe die Cluster Ressourcen optimal zu nutzen.
Die durchgef\"uhrten Experimente mit Lasso Linear Regression zeigen eine gesteigerte Leistung im Vergleich zu modernen Datenverarbeitungssystemen wie Apache Spark und reduzieren gleichzeitig den Aufwand Algorithmen zu entwickeln, parallelisieren und damit im gro{\ss}en Ma{\ss}stab zu experimentieren.
\end{abstractger}
