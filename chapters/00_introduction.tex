\chapter{Introduction}
Processing large amounts of data in order to solve a computational problem was always at the heart of distributed computing. Only the definition of what is considered "large" has changed over time. Processing of data that could not fit into a single machine was often reserved to organizations owning a supercomputer and therefore of limited use. Over the past decade the need for processing large amounts of data increased rapidly. Many companies and platforms discovered the value inherent to data and started collecting vast amounts, e.g. user activities or sensor data. Others have been collecting data for decades but have not been able to make use of it due to the huge costs of extended processing capabilities. The success of what is now commonly refered to as "Big Data" started with the introduction of MapReduce (c) by Google in 2004. This framework made it possible to process data in a distributed in fault tolerant way with the help of a compute cluster consisting of hundredth or thousandth of machines. The advantage over previous approachs is that the framework could be run entirely on top of machines using commodity hardware. Which does not require special hardware and therefore equals low cost. Since then, many of today's successful companies rely heavily on the ability to process vast amounts of data to improve their services and customer experience and it is used throughout many diverse fields ranging from finance, commerce all the way to healthcare and many more. This alone is a challenge by itself and over the past years many new frameworks have been introduced to make development and execution of data mining and machine learning algorithms considerably easier.

\section{MapReduce and Beyond}
Google's MapReduce has lead to a widespread adoption of big data processing and the increased awareness of the value of big data. MapReduce eventually lead to the Apache Hadoop project which quickly gained widespread adoption and by now provides a whole ecosystem around big data processing. Including a fault tolerant distributed filesystem, MapReduce framework and ....
(Explain in more detail what MapReduce is!, with pictures)
Google introduced its MapReduce system in 200x (c), with Apache Hadoop, Apache Spark (c), Apache Flink (c) and GraphLab (c) being the most widely used frameworks by the time. The first three frameworks are based upon the data-flow paradigm (c), whereas the latter uses a graph abstraction to model particular algorithms. While the primary focus of these systems lies on ETL and graph processing, they are often used to run machine learning algorithms as well. While this works well for algorithms that can be parallelized efficiently, performance on algorithms requiring many fast and probably asynchronous iterations to refine a model is not satisfying (c)(such as Logistic Regression or Latent Dirichlet Allocation). This fact led to the development of specialized frameworks, such as Petuum (c), ParameterServer (c) or Yahoo!LDA. These frameworks inhabit a completely different paradigm, not based on dataflow. While these frameworks are well suited for their particular purpose, namely ETL (c, fn) and execution of graph algorithms, when it comes to machine learning, these systems have some disadvantages, while these systems are very good at embarrassingly parallel algorithms and  namely the which led to poor performance when executing algorithms that quickly execute iterative model refinements (such as Logistic Regression) (often referred to as iterative convergent algorithms) due to large iteration overhead and the lack of executing iterations asynchronously on different nodes, which led to the invention of new systems. While these systems often increase the performance on machine learning algorithms by an order of magnitude compared to dataflow systems, most systems come with either limited usability which makes it difficult to implement additional algorithms, are tied to a specific algorithm or are very low level frameworks. Most recognized frameworks are Petuum (c), ParameterServer (c) and Yahoo!LDA (c).
During the past couple of years the amount of data to be processed by machine learning algorithms has increased steadily. Machine learning has become the core of many successful businesses and continues to play an important role in the artificial intelligence community. Making products smarter and tied to users needs. Since the introduction of Google MapReduce a lot of new frameworks for large scale data mining have been introduced to make the processing of vast amounts of data more easier and more efficient. Relying on the capability to process vast amount of data in a constrained setup (time, space). Data collected has immense value.
Efficiently distributing machine learning algoritms remains and extremely challenging problem. This motivates our work in the area of distributed machine learning. Ease of implementing new algorithms, 


\section{Distributed Machine Learning}
Machine learning has gained a lot of attention lately because it enables the possibility to learn from the collected data and works best with lot of data to learn from. Which is given by the vast amoutn of data collected. 
Improving speed, allowing more degrees of freedom (async program flow), ease the development of algorithms close to single machine implementations, compiler, letting the developer and researcher choose the strategy if necessary or implement heuristics <- challenge and motivation

\section{Thesis Outline}
introducing a novel system for distributed machine learning and researching mechanism to dynamically adapt and manage consistency among distributed state
describe each chapter

In this thesis we introduce a novel system for large scale distributed machine learning, which 