%!TEX root = ./../thesis.tex

\chapter{Introduction}
One of the most challenging tasks in computer science and engineering resolves around improving algorithm performance.
In general this has been done by making hardware faster and inventing new strategies and algorithms to parallelize work more efficiently.
Since it is clear that Moore's Law will not hold anymore, a lot of effort has been spent to horizontally scale algorithm computation across multiple machines.
Machine learning is no exception and efficient parallelization is a key aspect towards more intelligent systems.
By now, many general purpose frameworks for large scale data processing have been developed and published. Many of those are used for running more complex machine learning algorithms at scale as well.
Unfortunately, the performance often is not satisfying due to the architecture and programming model not reflecting the underlying structure of most commonly used machine learning algorithms.
Common data processing tasks can be represented as an extract-transform-load (ETL) pipeline, which is often easily parallelizable. This does not hold for machine learning, where algorithms are mostly sequential in nature and the only way of enabling parallel computation is by exploiting their inherent stochastic properties. This allows to break the sequential execution in favor of parallel learning on subsets, which then needs to be combined in order to obtain a global solution to the task.
While this can lead to a great speedup in terms of the amount of data processed, it can have a negative affect on the learning progress.
Therefore a key part of horizontally scaling machine learning algorithms is to ensure all participating learners have a consistent view on each others progress while at the same time maintaining a trade-off between communicating progress and spending time on their own learning task.


\section{MapReduce and Beyond}
Many of todays successful businesses throughout fields like finance, e-commerce and health-care rely heavily on the ability to process vast amounts of user or sensorical data, collected to make services smarter and user experience better.
In order to learn from the collected data, discover patterns and ultimately gain new insights, it needs to be processed by an algorithm.
It is not uncommon that the input ranges somewhere between hundred gigabytes and tenth of petabytes.
In the past, processing this much data required either a supercomputer, which was only available to large institutions or government entities or some proprietary compute cluster.
All this changed when Google introduced MapReduce \cite{Dean2004} in 2004.
The MapReduce framework made it possible to process data in a distributed and fault tolerant way with the help of a compute cluster formed by hundredth or thousandth of machines.
Instead of using a single, expensive, special hardware supercomputer the framework provides the foundation to assemble commodity hardware machines into a compute cluster.
The framework takes care of all necessary aspects to ensure a fault tolerant and parallel execution of a task submitted to the cluster.
The advantage compared to previous approaches is that the framework can be run entirely on top of machines using commodity hardware, which does not require special hardware and therefore equals low cost.

MapReduce essentially led the path to a convenient and widespread use of big data processing, which found its open source implementation in the Apache Hadoop project \cite{hadoop2009hadoop}.
The project quickly gained traction and has spawned many business grade platforms, which quickly gained widespread adoption and by now provides a whole ecosystem around big data processing. The software stack includes a fault tolerant distributed file-system (HDFS) a MapReduce framework and a cluster resource manager (YARN) \cite{KumarVavilapalli2013}.
On the other hand, MapReduce suffers from some practical limitations that lead to the development of new, more sophisticated and specialized big data frameworks. With the most widely used frameworks being Apache Spark \cite{Zaharia2010}, Apache Flink \cite{Alexandrov2014} and GraphLab \cite{Low2012}.
The first two frameworks use at it's core a data-flow pipeline based architecture, whereas the latter uses a graph abstraction to model particular algorithms.
All this works well for algorithms that can be expressed as an extract-transform-load (ETL) pipeline and are often embarrassingly parallel in nature.
On the other hand, machine learning algorithms often rely heavily on many, computationally light, iterations to iteratively update a shared state (model) such as logistic regression or Latent Dirichlet Allocation (LDA).
These so called iterative-convergent algorithms required a change in how systems for distributed machine learning operate at its core.


\section{Distributed Machine Learning}
This limitation essentially sparked the development of specialized frameworks for distributed execution of iterative-convergent algorithms used in common machine learning tasks.
The most widely recognized systems are Petuum \cite{Xing2015}, ParameterServer \cite{Li2014} and MALT \cite{Li2015}.
Different from the MapReduce paradigm, these frameworks, instead of using a pipeline to transform an immutable dataset into another immutable dataset, operate on a fixed but mutable state, which is held by a single machine or distributed over multiple machines.
This state can then be updated by workers that have computed an update locally and send it to these state keepers. Which act as surrogates, taking state updates and incorporating these into the state by some user defined function.
While these systems can increase the performance on machine learning algorithms by an order of magnitude \cite{Xing2015} compared to data-flow systems, most systems come with either limited usability, which makes it difficult to implement additional algorithms, are tied to a specific algorithm or are very low level frameworks.
Efficiently distributing machine learning algorithms while at the same time provide the ability to concisely express machine learning algorithms remains an extremely challenging problem.
A system targeting the execution of those algorithms at scale must therefore provide the ability to concisely express the underlying algebraic structure and at the same time be flexible enough to allow experimentation and fine tuning.
Where consistency management is an essential part to ensure that algorithms are executed fast and efficient.


\section{Thesis Outline}
In this thesis we introduce a novel framework for large scale distributed machine learning.
It improves upon currently available systems by providing a powerful programming abstraction that can concisely express state of the art machine learning algorithms and at the same time minimizes the effort necessary to move from a single machine to a cluster.
The framework design is optimized for efficient parallel asynchronous execution of iterative-convergent algorithms in a cluster and ensures the required consistency is enforced among parallel learners, depending on the algorithm properties. By allowing the user to decide how to maintaining the best trade-off between algorithm execution and progress communication the performance is improved as well.
All of this can be easily customized for quick prototyping and fine tuning, which makes the system suited for developers as well as researchers.
The goal of this thesis is to implement the state centric programming model and show its performance in comparison with Apache Spark on an example implementation of the CoCoA \cite{Jaggi2014} framework.
I start off by providing a background on the architecture and inner workings of state of the art frameworks for big data processing and distributed machine learning in Chapter \ref{c:background}. Furthermore the most commonly used algorithms and optimization techniques are introduced.
The majority of those algorithms can be classified into the group of so called iterative-convergent algorithms for which I am going to provide a more formal description. I introduce the common algorithm parallelization strategies in distributed machine learning.
I conclude the chapter by providing an overview over the challenges and issues that need to be addressed and considered when developing a distributed machine learning system and how this is achieved by current frameworks.
This will give rise to understanding why a different framework architecture and abstraction is necessary to improve the performance and expressibility of large scale distributed machine learning applications.
Chapter \ref{c:state_centric} therefore introduces the state centric programming model, which treats the state as a mutable first class citizen, which can be distributed and altered by updates that result from distributed computation.
This allows the system to reason about the most optimal distribution of state in the cluster which is then scheduled with computation that can update the state. I further describe the architecture of the system and how the essential components are implemented.
When updating a shared state from multiple different locations, consistency must be maintained in order to ensure the algorithm progresses as expected. The system is responsible for managing a state's consistency among all of its instances across the cluster. Chapter \ref{c:consistency_mgmt} therefore describes several schemes for ensuring consistency and at the same time optimizing for bandwidth and computational resource usage.
In order to show the system and its consistency management in action, Chapter \ref{c:experiments} compares the system against Apache Spark by running the CoCoA framework on two datasets with elastic-net regularized linear regression as the chosen algorithm.
Chapter \ref{c:conclusion} summarize the experiments with the lessons learned and provides suggestions on how to further improve systems for large scale distributed machine learning.
