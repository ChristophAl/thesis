\chapter{Background}
\label{c:background}

\section{Iterative Convergent Algorithms}
Most analytics used in large scale machine learning such as topic models, matrix factorization or neural networks employ either gradient based methods or Markov Chain Monte Carlo methods. Those algorithms try to iteratively optimize an objective \textit{$O(W, D)$}, quantifying the fit of a model \textit{$W$} given the data \textit{$D$}. Depending on the use case, the objective is also referred to as cost or loss. Commonly used objectives are the reconstruction loss or likelihood. At each iteration \textit{t} an updated model \textit{$W^{t}$} is computed based on the model of the previous iteration \textit{$W^{(t-1)}$} and the training data \textit{D}. The resulting model \textit{$W^{t}$} is again a better summary of the data \textit{D} under the objective \textit{O}. Eq. \ref{eqn:grad_upd} shows the process of refining the model, with $\Delta$ being an arbitrary update function.
\begin{equation}
W^{t} = W^{(t-1)} + \Delta(W^{(t-1)}, D)
\label{eqn:grad_upd}
\end{equation}
The update function depends on the algorithm employed by the analytic and can be seen as a procedure of obtaining a step towards a better model. At each iteration an update step is computed and applied to the latest model until a stop condition is satisfied. The change of the overall model or the difference in loss between two iterations is monitored. If the difference is below a certain threshold the computation stops and the algorithm is said to be converged.

The most important part of any distributed system is the synchronization strategy used to ensure consistency among multiple nodes concurrently accessing and updating the parameters stored on the parameter server. There are multiple schemes to synchronize nodes during the iterative parameter refinement. \textit{Bulk synchronous parallelization (BSP)} leads to the best algorithm throughput (e.g. convergence achieved over the number of data points processed). Essentially each worker must finish its iteration and push all updates to the parameter server. The server then computes a refined model according to Eq. \ref{eq:bsp_upd} and each node retrieves the updated parameters before beginning the next iteration. This synchronization scheme guarantees consistency among all nodes at all times.
\begin{equation}
W^{t} = W^{(t-1)} + \frac{1}{K}\sum_{k=1}^{K}\Delta(W^{(t-1)}_{k}, D_{k})
\label{eq:bsp_upd}
\end{equation}
While this synchronization strategy essentially recovers the sequential algorithm for a single machine and has the same convergence properties and guarantees, it suffers from a severe limitation when used in a distributed setup \cite{langford2009slow}. Imagine one of the workers is for some reason a lot slower than the others. Due to the synchronization strategy, the other workers have to wait for this particular worker to complete its iteration. This is well known as the straggler problem \cite{ananthanarayanan2013effective} and can seriously affect performance in a distributed environment, because the progress is limited by the slowest node in the cluster.
The second strategy is \textit{total asynchronous parallelization (TAP)}. Similar to BSP, all nodes push their parameter updates to the server after each iteration but in this case, the changes are applied to the model immediately. No waiting for other workers is required, resulting in a very high data throughput. The straggler problem can be mitigated by this synchronization scheme as well, depicted in Figure \ref{fig:workers}. Even though worker 3 is a straggler, the remaining workers can continue with their next iterations without waiting for slower workers. Although this consistency scheme seems to work quite well in practice \cite{li2014scaling}, it lacks formal convergence guarantees and can even diverge \cite{dai2014high}. The reason is that no bound exists for a situation where the divergence in iterations between the slowest and the fastest worker is unbound.
A middle ground between bulk synchronous parallelization and total asynchronous parallelization is \textit{stale synchronous parallel (SSP)} \cite{ho2013more} or \textit{bounded staleness (BS)}. As shown in Figure \ref{fig:bsp_workers}, BSP introduces a maximum delay, or staleness threshold, of $\Delta_{max}$ between the slowest and fastest node. This overcomes the limitation of the TAP approach by introducing a bound on the number of iterations. Formal convergence guarantees can be restored while still maintaining the flexibility of asynchronous parallelization and limiting the straggler problem \cite{cipar2013solving}.

\section{Dataflow Systems}

\section{Distributed Machine Learning}

