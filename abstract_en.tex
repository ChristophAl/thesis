%!TEX root = ./thesis.tex
% $Log: abstract.tex,v $
% Revision 1.1  93/05/14  14:56:25  starflt
% Initial revision
% 
% Revision 1.1  90/05/04  10:41:01  lwvanels
% Initial revision
% 
%
%% The text of your abstract and nothing else (other than comments) goes here.
%% It will be single-spaced and the rest of the text that is supposed to go on
%% the abstract page will be generated by the abstractpage environment.  This
%% file should be \input (not \include 'd) from cover.tex.
In recent years, machine learning emerged as the core of many successful applications and businesses.
While the rise of artificial intelligence continues, the amount of data to be processed grows even faster.
Unsurprisingly a lot of research has since focused on methods to parallelize commonly used algorithms and new systems and frameworks have been published, trying to improve the performance of distributed machine learning.
Even though there has been a lot of progress towards more efficient systems, many state-of-the-art systems still have limitations.
Current frameworks are neither expressible nor flexible enough to allow for efficient development of distributed machine learning algorithms, making them unsuited for experimentation and quick prototyping even though this is essential for achieving optimal performance.
On the other hand, most parallelization strategies exploit the algorithms inherent stochastic nature to enable parallel execution at the expense of lowered consistency among the shared state.
Even though this does not necessarily affect quality of the results, an improper chosen level of consistency can severely affect algorithm performance, resulting in a non optimal convergence rate and therefore increased runtime.

This thesis presents the prototype of a novel framework for distributed machine learning called ExaML, which uses a state centric programming abstraction with yield semantics.
The result is a programming model that allows the user to focus on the key elements of developing distributed machine learning algorithms, namely progress communication and consistency management among distributed workers.
The framework also takes care of utilizing the cluster resources in an optimal fashion.
Experiments using lasso linear regression show an increased performance compared to state-of-the-art data processing systems like Apache Spark and at the same time reduce the effort of developing, parallelizing and experimenting with distributed machine learning algorithms at scale.
