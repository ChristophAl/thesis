%!TEX root = ./thesis.tex
% $Log: abstract.tex,v $
% Revision 1.1  93/05/14  14:56:25  starflt
% Initial revision
% 
% Revision 1.1  90/05/04  10:41:01  lwvanels
% Initial revision
% 
%
%% The text of your abstract and nothing else (other than comments) goes here.
%% It will be single-spaced and the rest of the text that is supposed to go on
%% the abstract page will be generated by the abstractpage environment.  This
%% file should be \input (not \include 'd) from cover.tex.
In recent years, machine learning emerged as the most important part of many successful applications and businesses.
While the rise of artificial intelligence continues, the amount of data to be processed grows even faster.
Unsurprisingly a lot of research has since focused on methods to parallelize commonly used algorithms and new systems and frameworks have been published to improve the performance of distributed machine learning.
Even though there has been a lot of progress towards more efficient systems, many state-of-the-art systems still have limitations.
Current frameworks are neither expressible nor flexible enough to allow efficient development of distributed machine learning algorithms, which makes them unsuited for experimentation and quick prototyping even though this is an essential part to optimimal performance.
On the other hand, most parallelization strategies exploit the algorithms inherent stochastic nature to allow for parallel execution at the expense of lowered consistency among the distributed data structures.
Even though this does not necessarily affect quality of the results, an improper choosen level of consistency can severely affect algorithm performance, resulting in a non optimal convergence rate and therefore increased runtime.

This thesis introduces a novel framework for distributed machine learning, which is based on a state centric programming model with yield semantics.
The programming model allows the user to focus on the key elements of developing distributed machine learning algorithms, namely update communication, update merging and consistency management among distributed workers, while the system takes care of distributing the computation in an optimal fashion.
The experiments using elastic-net regularized linear regression show an increased performance compared to state-of-the-art data processing systems like Apache Spark and at the same time reduce the effort of developing, parallelizing and experimenting with distributed machine learning algorithms at scale.
